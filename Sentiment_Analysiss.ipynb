{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyA2y4ESHCxFhxwXcUX3Qa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nils1511/Sentiment_Analysis/blob/main/Sentiment_Analysiss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "some important data preprocessing steps:\n",
        "\n",
        "1. The dataset has about 183,500 rows of data. There are 1147 null values. I simply will get rid of those null values.\n",
        "2. As the dataset is pretty big, it takes a lot of time to run some machine learning algorithm. So, I used 30% of the data for this project which is still 54,000 data. The sample was representative.\n",
        "3. If the rating is 1 and 2 that will be considered a bad review or negative review. And if the review is 3, 4, and 5, the review will be considered as a good review or positive review. So, I added a new column named ‘sentiments’ to the dataset that will use 1 for the positive reviews and 0 for the negative reviews."
      ],
      "metadata": {
        "id": "1wFIFhs626gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://d396qusza40orc.cloudfront.net/phoenixassets/amazon_baby.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAC9Mwk889Kv",
        "outputId": "ef8b8a56-b74b-492c-ac44-fbf47bf8b9b2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-17 05:28:36--  https://d396qusza40orc.cloudfront.net/phoenixassets/amazon_baby.csv\n",
            "Resolving d396qusza40orc.cloudfront.net (d396qusza40orc.cloudfront.net)... 18.155.174.183, 18.155.174.134, 18.155.174.64, ...\n",
            "Connecting to d396qusza40orc.cloudfront.net (d396qusza40orc.cloudfront.net)|18.155.174.183|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89444990 (85M) [text/csv]\n",
            "Saving to: ‘amazon_baby.csv’\n",
            "\n",
            "amazon_baby.csv     100%[===================>]  85.30M   176MB/s    in 0.5s    \n",
            "\n",
            "2023-08-17 05:28:36 (176 MB/s) - ‘amazon_baby.csv’ saved [89444990/89444990]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmGZI9BQ9M8-",
        "outputId": "a44c559c-8f1c-4aec-f5b4-b826e0dd25ee"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amazon_baby.csv\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LH-KhCh12BrU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('amazon_baby.csv')\n",
        "#getting rid of null values\n",
        "df = df.dropna()\n",
        "import numpy as np\n",
        "np.random.seed(34)\n",
        "#Taking a 30% representative sample\n",
        "df1 = df.sample(frac = 0.3)\n",
        "#Adding the sentiments column\n",
        "df1['sentiments'] = df1.rating.apply(lambda x: 0 if x in [1, 2] else 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Define the input features and the labels"
      ],
      "metadata": {
        "id": "R1WLNmXT3vM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1['review']\n",
        "y = df1['sentiments']"
      ],
      "metadata": {
        "id": "iJMol5nx2P__"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the steps.\n",
        "\n",
        "1. The first step is to split the dataset into training sets and testing sets.\n",
        "2. Vectorize the input feature that is out review column (both training and testing data)\n",
        "3. import the model from scikit learn library.\n",
        "4. Find the accuracy score\n",
        "5. find the true positive and true negative rates"
      ],
      "metadata": {
        "id": "cSXSd_Uv3yym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "\n",
        "\n",
        "test_size = 0.5, random_state=24)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "#Vectorizing the text data\n",
        "ctmTr = cv.fit_transform(X_train)\n",
        "X_test_dtm = cv.transform(X_test)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#Training the model\n",
        "lr = LogisticRegression()\n",
        "lr.fit(ctmTr, y_train)\n",
        "#Accuracy score\n",
        "lr_score = lr.score(X_test_dtm, y_test)\n",
        "print(\"Results for Logistic Regression with CountVectorizer\")\n",
        "print(lr_score)\n",
        "#Predicting the labels for test data\n",
        "y_pred_lr = lr.predict(X_test_dtm)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#Confusion matrix\n",
        "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_lr).ravel()\n",
        "print(tn, fp, fn, tp)\n",
        "#True positive and true negative rates\n",
        "tpr_lr = round(tp/(tp + fn), 4)\n",
        "tnr_lr = round(tn/(tn+fp), 4)\n",
        "print(tpr_lr, tnr_lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvqRs3Yi2Tmh",
        "outputId": "8673f199-2ac7-4f9e-c585-e90e3e7b33c7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Logistic Regression with CountVectorizer\n",
            "0.9027706703706412\n",
            "2336 1607 1053 22362\n",
            "0.955 0.5924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see I have the print statement to print accuracy, true positive, false positive, true negative, false negative, true negative rate, and false-negative rate."
      ],
      "metadata": {
        "id": "q7-0EWVF4UUW"
      }
    }
  ]
}